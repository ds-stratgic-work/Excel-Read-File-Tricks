{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd0e62d-c4b1-477d-b363-6a375bc1b965",
   "metadata": {},
   "source": [
    "# A 100x Faster Way To Read Excel Files with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba70414-1546-4faa-be8a-1250b1c74473",
   "metadata": {},
   "source": [
    "Python users often use Excel files to store and retrieve data, as it is a preferred format for data sharing among business professionals. However, working with Excel files in Python can be slow and inefficient. To overcome this issue, we have compiled a list of five alternative methods for loading data in Python, which can significantly improve data processing speed. By implementing these techniques, we can expect lightning-fast data processing, with an acceleration of up to 3 orders in magnitude. \n",
    "\n",
    "Consider a scenario where we aim to import 10 Excel files containing 30000 rows and 31 columns each, resulting in a total size of approximately 80MB. This serves as a typical example of where we need to load transactional data from an ERP system (SAP) into Python for further analysis.\r\n",
    "\r\n",
    "To accomplish this, we will generate simulated data and import the necessary libraries. The usage of pickle and joblib will be elaborated on in subsequent sections of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2811724-194b-4135-aa43-1d9057c25330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "for file_number in range(10):\n",
    "    values = np.random.uniform(size=(30000,31))\n",
    "    pd.DataFrame(values).to_csv(f\"Dummy {file_number}.csv\")\n",
    "    pd.DataFrame(values).to_excel(f\"Dummy {file_number}.xlsx\")\n",
    "    pd.DataFrame(values).to_pickle(f\"Dummy {file_number}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6da33-12cd-4604-9f9d-2753adfe174a",
   "metadata": {},
   "source": [
    "## 1.Starting with the First option to Load Data\n",
    "We will begin by simply loading these files. Initially, we will establish a Pandas Dataframe and subsequently merge each Excel file into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab1674-f1ea-4bd7-b5cf-d73e6d9acba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df = pd.read_excel(“Dummy 0.xlsx”)\n",
    "for file_number in range(1,10):\n",
    " df.append(pd.read_excel(f”Dummy {file_number}.xlsx”))\n",
    "end = time.time()\n",
    "print(“Excel:”, end — start)\n",
    "\n",
    "\n",
    ">> Excel: 53.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde28e51-b2e5-4c7e-ac81-0db913808437",
   "metadata": {},
   "source": [
    "## 2. The second option is to Read .CSV file\n",
    "\n",
    "Let us consider saving these files in .csv format instead of .xlsx from our ERP/System/SAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2baf269-cfa9-4c2d-95a5-5cdb433f1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df = pd.read_csv(“Dummy 0.csv”)\n",
    "for file_number in range(1,10):\n",
    " df.append(pd.read_csv(f”Dummy {file_number}.csv”))\n",
    "end = time.time()\n",
    "print(“CSV:”, end — start)\n",
    "\n",
    "\n",
    ">> CSV: 0.632"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64796061-f4b6-4130-bba5-ae2f1f03561f",
   "metadata": {},
   "source": [
    "The loading time for these files is 0.63 seconds, which is almost 10 times quicker!\r\n",
    "\r\n",
    "Python demonstrates a loading speed that is 100 times faster with CSV files compared to Excel files. Let's opt for CSVs.\r\n",
    "\r\n",
    "One drawback is that CSV files tend to be larger than .xlsx files. For instance, in this scenario, .csv files are 9.5MB, while .xlsx files are 6.4MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a36bc1-22b5-4cd1-ab86-134156e106ca",
   "metadata": {},
   "source": [
    "## 3. The third option is to create a Pandas DataFrame\n",
    "\n",
    "We can enhance the efficiency of our process by altering our approach to creating pandas DataFrames. Rather than adding each file to an already existing DataFrame, we can load each DataFrame separately into a list and subsequently merge the entire list into a single DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a296e-c8e6-43ed-88f3-51a23b9d1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df = []\n",
    "for file_number in range(10):\n",
    " temp = pd.read_csv(f”Dummy {file_number}.csv”)\n",
    " df.append(temp)\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "end = time.time()\n",
    "print(“CSV2:”, end — start)\n",
    "\n",
    "\n",
    ">> CSV2: 0.619"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f731b-39d7-4d4d-9e0d-7b8aedcb4631",
   "metadata": {},
   "source": [
    "We have decreased the time by a small percentage. This technique may prove to be beneficial when handling larger Dataframes (df >> 100MB). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869174a-4e7b-4a9e-af52-b5ac8ed3cfa5",
   "metadata": {},
   "source": [
    "## 4. Utilize Joblib for Concurrent CSV Imports\r\n",
    "We aim to import 10 files in Python. Rather than importing each file sequentially, consider importing them simultaneously in parallel. This can be achieved effortlessly through the utilization of joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975e473-6dae-4c4a-b80b-83f169949078",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "def loop(file_number):\n",
    " return pd.read_csv(f”Dummy {file_number}.csv”)\n",
    "df = Parallel(n_jobs=-1, verbose=10)(delayed(loop)(file_number) for file_number in range(10))\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "end = time.time()\n",
    "print(“CSV//:”, end — start)\n",
    "\n",
    "\n",
    ">> CSV//: 0.386"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4ba1e-7a11-4046-a59a-1e8042c596b6",
   "metadata": {},
   "source": [
    "The performance is almost two times faster compared to the single-core version. Nevertheless, it is important to note that utilizing 8 cores may not necessarily result in an eightfold increase in speed (in this case, I experienced a two-times speed increase by utilizing 8 cores on a Mac Air equipped with the new M1 chip)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e40b8-1431-4b6e-8e98-61cb38b92534",
   "metadata": {},
   "source": [
    "## Python Parallelization Made Easy with Joblib\r\n",
    "Joblib is a straightforward Python library that enables the execution of a function in parallel. In essence, joblib functions similarly to a list comprehension, with the distinction that each iteration is carried out by a separate thread. Let's take a look at an illustrative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46dc80-5cee-48cd-9468-17fe2e662ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(file_number):\n",
    " return pd.read_csv(f”Dummy {file_number}.csv”)\n",
    "df = Parallel(n_jobs=-1, verbose=10)(delayed(loop)(file_number) for file_number in range(10))\n",
    "#equivalent to\n",
    "df = [loop(file_number) for file_number in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a34db2-b909-4a29-9d01-219007912325",
   "metadata": {},
   "source": [
    "## 5. The Fifth option is to use pickle\n",
    "Storing data in pickle files, a specific format utilized by Python, can significantly increase processing speed compared to using .csv files.\n",
    "\n",
    "###### However, it is important to note that pickle files cannot be easily opened and viewed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaae709-3e63-410c-87cd-238dcafc4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "def loop(file_number):\n",
    " return pd.read_pickle(f”Dummy {file_number}.pickle”)\n",
    "df = Parallel(n_jobs=-1, verbose=10)(delayed(loop)(file_number) for file_number in range(10))\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "end = time.time()\n",
    "print(“Pickle//:”, end — start)\n",
    "\n",
    "\n",
    ">> Pickle//: 0.072"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd20a09-860c-4d6b-8c8d-320fa4c3c5a9",
   "metadata": {},
   "source": [
    "##### We have managed to reduce the run time by 80%!\n",
    "\n",
    "- In general, working with pickle files is much faster compared to csv files. \n",
    "\n",
    "- It is not possible to directly extract data from a system in pickle files.\n",
    "\n",
    "Pickles can be used in the following scenarios:\n",
    "\n",
    "1. If we want to save data from one of our Python processes (without the intention of opening it in Excel) for future use or\n",
    "2. in another process, it is advisable to save our Dataframes as pickles instead of .csv files.\n",
    "\n",
    "3. If we need to reload the same file(s) multiple times, it is recommended to save the file as a pickle the first time we open it. This way, we can directly load the pickle version in subsequent instances.\n",
    "\n",
    "4. It is important to note that pickle files tend to occupy more space on your drive (excluding this specific example).\n",
    "\n",
    "5. For instance, let's consider a scenario where we work with transactional monthly data. Each month, we load a new set of data. In this case, we can save all the historical data as .pickle files. Whenever we receive a new file, can be loaded once as a .csv file and then saved as a .pickle file for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254671aa-5734-4771-868e-f5ef05fb5b25",
   "metadata": {},
   "source": [
    "## In Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e97043-b195-484e-9cb5-56fae84784e8",
   "metadata": {},
   "source": [
    "Let us consider a scenario where you have received Excel files and we are left with no alternative but to load them in their current state. Additionally, we have the option to utilize joblib for parallelization. In comparison to our previous pickle code, the only modification required is to update the loop function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598b57a-cddc-4c34-9c30-5ace61f40709",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "def loop(file_number):\n",
    "    return pd.read_excel(f\"Dummy {file_number}.xlsx\")\n",
    "df = Parallel(n_jobs=-1, verbose=10)(delayed(loop)(file_number) for file_number in range(10))\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "end = time.time()\n",
    "print(\"Excel//:\", end - start)\n",
    "\n",
    "\n",
    ">> 13.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c21e0-7d17-4a63-8d21-02824c5fb61e",
   "metadata": {},
   "source": [
    "We can decrease the loading time by 70%, bringing it down from 50 seconds to just 13 seconds. Additionally, you have the option to utilize this loop for generating pickle files instantly. This will enable you to experience significantly faster loading times when you access these files in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559dbe0-e1a6-4ff7-b23d-9e80a168527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(file_number):\n",
    "    temp = pd.read_excel(f\"Dummy {file_number}.xlsx\")\n",
    "    temp.to_pickle(f\"Dummy {file_number}.pickle\")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e27c82-8943-4eca-a9a3-496270311ef5",
   "metadata": {},
   "source": [
    "##### - Joblib provides the option to adjust the parallelization backend to reduce unnecessary overhead. Simply specify \n",
    "- prefer=\"threads\"\n",
    "  when utilizing Parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c5f5c-e408-467a-a43b-f13e04128a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(file_number):\n",
    "      return pd.read_pickle(f\"Dummy {file_number}.pickle\")\n",
    "  df = Parallel(n_jobs=-1, verbose=0, prefer=\"threads\")(delayed(loop)(file_number) for file_number in range(10))\n",
    "  df = pd.concat(df, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
